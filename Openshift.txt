Exam : Red Hat Certified Specialist in Containers and Kubernetes exam (EX180)
openshift sandbox creds : meet14764 , Gogi@12345

Openshift training ----------------------------- 

1. The openshift container platform  or OCP is some thing that runs the applications.
2. Many resources are there which helps developers to deploy their applications to the ocp clusters and expose it to the end users. 
3. The openshift runs on multiple openshift cluster servers or kubernetes cluster .
4. There are two categories or Planes in OCP servers : 
	a. Control plane : 
		- which contains the Rest Api , Data persistence and monitoring .
	b. Data plane : 
		- which contains the servers where the applications runs.
		
								--------------		       							----------------
								Control plane 				Deploy apps to 					
Developers ---------------->	 Rest api 				----------------------->		 Data plane 
																						ocp servers
			Send request																
			for our apps 		--------------		       							----------------	
			
			
			
Various tools provided By either redhat named as the sandbox or the desktop app to run the openshift which parallely runs the Control planes and the data planes.			


5. OCP cli  : its the command line interface to run the cmmand to deploy your app , manage the pods , containers , monitoring and logging the app logs etc.

6. OCP dashboard can also used to manage the same as we do it in OCL cli . but we have an advantage to automate using OCP cli code and make the deployment , auto config , autoscaling easy.

7. OpenShift does have a built-in image registr which is called Imagestream.

8. In OCP it uses Pods for everything like for creating a Deployement config if will spin up the pod for it once completes it will delete , just like for creating a build cofig it will spin up the pod for it and once completes it will delete it.

9. Its PAAS as a service . Means u just have to run you application in the service that Redhat OCP servers provides . No need to maintain the underlying infra and OS.

10. open shift is build on top of the kubernetes which is on top of dockers . OCP  provide various tools and resources that abstract the underlying functionality of kuberneter and docker . Those tools may include Cli , web console  to interact and r brig your apps up and running .

11. We can use either the MInishift or openshift sandbox . MInishift its the one we used in OPtum (where we install Minishift on different linux boxes and 
have the openshift url for differnet Dcs) is the iso image we can download by running MInishift.exe and open that iso image in virtual box but that will take 
more memory of underlying machine . So go for  the sandbox  go to https://developers.redhat.com/developer-sandbox

12. In minishift we have different numbers of catalog which shows you can directly enter the github or gitlab or source code projct url and allow the deployment take place .
		The catalog contains many things , like python , jenkins pipeline  , mysql , ruby , nginx , ruby etc.

Note openshift 4 is the latest version in pursuit by Redhat.
------------------------------------------------------------------------------------------------------------------

Installion of dev tools to start working with the cli.

1. Docker Desktop  https://www.docker.com/get-started : to get the installer. to get the docker desktop. This will give you the docker cli also for your desktop to start playing wih container.

2. Openshift installtion : hosted sandbox env is better option to gowith .
	1. Create a redhat account using redhat.com
	2. go to https://developers.redhat.com/developer-sandbox
	3. Click on the ? on top right corner post going inside sandbox dashboard.
	4. Download the windows zip , unzip it , place it in a User folder 
	5. copy the path of the extracted folder till oc.app file and add it to the ENV variable ---> user variables 
	6. open a terminal and type oc olaaaaaaa ! it started recognising.
	
	NOTE *** : THE ABOVE STEPS WILL LEAD TO CREAT THE OPENSHIFT DASHBOARD SIMILAR TO WHAT WE DID BEFORE AND FROM THERE WE CAN COPY THE LOGIN COMMAND AND USE THAT TO CONNECT OUR LOCAL SYSTEM TERMINAL WITH IT FOR INTERACTION.
	
	7. Now clone a code from https://gitlab.com/practical-openshift/labs to see the other steps .
	
3. Small overview on container : These are isolated light weight processes run in their own space utilizing small amount of underlying OS memory .
	Containers are created by running a packaged or bundled artefacts names as images .
	A container is just like a virtual machine running with its own os.
	Resources required for running the cotainer is Docker engine 
	Docker engine contains docker daemon process which receives all the requests to create a container from an image.
	Docker engine maintains the container lifecycle.
	
	
	Docker client (cli) ---> Send the request to -----> Docker Daemon ------> Itchecks for images locally if present ?-------->Yes ------> runs the image ------>Create container 
																									|
																									|
																									|
																									|
																									|
																									|------------------No -----------> Pulls the images from the docker repository and ------> create container 
																									
																									
																									---------------------------------------> post that will displays the output to the docker client -------> which shows it to the terminal.
																									
																									
4. Docker image build : 
	- This requires the Dockerfile which contains the steps to run the docker image
	- Then navigate to that Dockerfile and source code location
	- Run docker Build .   (dot means in current directoty)
	
	Some core docker commands 
		docker ps  ---------- will show the running containers
		docker ps -a  -------- will show the all running or stopped containers 
		docker images  ------- will show you the image build using dockerfile or downloaded from docker hub 
		docker images -a ----- this will show you all images 
		docker run imagename --- this will create a container 
		docker run -it imagename -- you will go into container in interactive mode(start and make connection).
		docker run -itd imagesname -- will send the running container to background and your cursor will on the base VM.
		docker run -itd -p 8080:8080 imagename -- will send the running container to background and your cursor will on the base VM. and map a port (host machine-port : container-port)
	docker file autopsy : 
		.FROM : is used to get the base image for your container 
		.COPY  : COPY fileinhostmachinelocation    filelocationinsidecontainer  : used to copy the file
		.ENV : ENV nameo_of_variabe "value to this variable"  : used to create a variable which is red inside the container.
		.RUN : the bachine machine run the command given to run command wait for it to finish and saved the results inside the container.
			
			Run chmod 777 /home/files/txt/run.txt
			****Note this commands is mostly used to change the folder access level , update the package in linux container etc
		.EXPOSE  : it is only used for documentation , but use port command to map the port of hostmachine to you container app.
		.CMD java -jar /src/lib/gateway.jat:snapshot_1.0
		
			
5. Projects and Users  :
	Projects are used by the OCP to grouped together the related resources.(dc,bc,service,route etc)
		oc delete all --all
		oc whoami : will give you the current user login 
		oc whoamo -t : we can use this command to get the token to login to console from cli. this command will work post logging in .
		oc logout
		oc login -u meet14764
		oc get users 
		oc project
		oc new-project myproject
		oc projects : list the projects in OCP cluster.
		oc status -- shows all resources in our project
			output of oc status :                								
				svc/hello-world - 172.30.180.208:8080							--------- Service
					dc/hello-world deploys istag/hello-world:latest             --------- Deployement config
						deployment #1 deployed 8 minutes ago - 1 pod			---------1 pod
		
6. Pod : contains one container or more than one container.		
	oc explain pod : to get the description of the pod
	oc explain pod.spec : spec is one of the field in pod
	oc explain pod.spec.containers : containers is a field inside spec.
	
	So explain help giving the documentation of the command.
	
	
-------------------------------------------------------POD RESOURCE----------------------------------------	
clone : https://gitlab.com/practical-openshift/labs
7. how to run / start a POD in OCP   oc explain pod.spec

Below commands sends a post or get request to the control plane (rest api) for creating pods and get the pods running status.

# Check that OpenShift is running 
# You can use minishift start if the cluster has stopped
oc status

# Create a Pod on OpenShift based on a file
oc create -f pods/pod.yaml   

# Show all currently running Pods
oc get pods
	
	
# In order to go inside of the pod from you local termnal 
oc rsh podname   ---------- the same terminal that we used on OCP dashboard inside the pod.

# to ping a service on a port 
wget localhost:8080 or curl localhost:8080	

Cat filename : to open a file in shell.

# Delete the pod
oc delete pod delete		


# To see the Real time updates in pods	
oc get pods --watch
			
above command will show logs in the terminal and help in monitoring the pods deployemnets.			
			
******NOTE****** : In simple language , you need to compile you app , build it and create an image , push it to the docker hub repo . Now use that image to create a pod in open shift , to create containers any where in kubernetes etc. open shift creates a POD even if you run a single container or 10000000 containers and we can also create multiple pods in bigger architecture.			
	If you give git url the bc will be created using new-app , but if you give the image url then as usual no build required and direct dc and svc will get created .
	In oc get -f logs bc/demo-app will show you the logs for bc you will see the image build and push also .



8. Yaml : standard language for docker config (yet another markup language)

https://yaml.org/

NOTE : SO WE HAVE BUILD AN IMAGE USING A DOCKERFILE , PUSHED THAT IMAGE TO DOCKER HUB , USED A YAML FILE TO PULL THAT IMAGE OUT , CREATED A POD OUT OF IT AND DEPLOYED IT ON OCP CLUSTER.


-------------------------------------------------------POD RESOURCE----------------------------------------

-------------------------------------------------------DEPLOYEMENT CONFIG RESOURCE----------------------------------------
9.Deployement configs :  oc explain deploymentconfig.spec
	Its collections of pods .
	
	- Deployment Configs define the template for a pod and manages deploying new
     images or configuration changes. A single deployment configuration is
     usually analogous to a single micro-service. Can support many different
     deployment patterns, including full restart, customizable rolling updates,
     and fully custom behaviors, as well as pre- and post- deployment hooks.
     Each individual deployment is represented as a replication controller
	 
	-Deployment config is a collections of pods.
	-The also falls under spec which is under template key 
	
command : 

	-Create a deployemnetconfig
		oc new-app quay.io/practicalopenshift/hello-world --as-deployment-config 
		oc get svc				------ will give service
		oc get dc 			  --------- will give deploymentconfig
		oc get istag          ------- image stream tag		
		oc delete svc/hello-world ------ dleete the service
		oc delete dc/hello-world  ----- delete deployment config.
	
	-Delete a deployemnetconfig
		We can also delete the resources using label  : to get the label we can use describe command 
		
		oc describe dc/hello-world -------- describe the dc with label 
		oc delete all -l app=hello-world ----------- then use -l and mention the label got from describe command 
		
		best recommended way to delete the resources.
			 
	-Name a deployemnetconfig using --name flag
		oc new-app quay.io/practicalopenshift/hello-world --name demo-app --as-deployment-config 
		oc status : 
			svc/demo-app - 172.30.210.218:8080
				dc/demo-app deploys istag/demo-app:latest
					deployment #1 running for 3 minutes - 1 pod


		NOte : we can deploy the same DC with different --name multiple copies on OCP.
		
	- Create deployment config using git repo url  : for this the git repository will have a DockerFile which it will use to build and image and use that to create a DC
		oc new-app https://gitlab.com/practical-openshift/hello-world.git --as-deployment-config
		
		Steps taken : 
			1. Will clone the repository
			2. go through the dockerfile step by step as below and build and image push it to image streams of buid config.
				for example : 
					FROM golang:alpine
					ADD src/hello-world.go hello-world.go					
					ENV MESSAGE "Welcome! You can change this message by replacing the MESSAGE environment variable."
					ENV HOME /go					
					RUN chgrp -R 0 /go && chmod -R g+rwX /go					
					EXPOSE 8080					
					LABEL io.openshift.expose-services 8080/http					
					USER 1001					
					CMD go run hello-world.go
					
		oc status : 
				svc/hello-world - 172.30.61.85:8080
					dc/hello-world deploys istag/hello-world:latest <-
						bc/hello-world docker builds https://gitlab.com/practical-openshift/hello-world.git on istag/golang:alpine   ---- extra step from above command we did 
						not built yet
							deployment #1 waiting on image or update


		NOte : here bc/hello-world is also there because oc uses the container to build the image by cloning the code from the dockerfile one step at a time. And new-app command only have one diff is that it using git repo url instead of image . if we give Imagetag it will not create BC as it is not required bcse image is already there .
		But here we have to build an image from the git repo. Thats why a Bc is created.

		oc logs -f  bc/hello-world  : to get the logs of bc : which will sho you the steps took to buld the image by cloning.
		
		
	- Replication controller :  use by deploy to mainatain proper number of pods running .
				oc get rc  : get the replication controller.
				
	- To get the yaml of any yaml : BC , DC , service , imagestream :  oc get -o yaml bc/hello-world	
	
	- Roll out a new version of DC post changing te config / src code etc .
				oc rollout latest dc/demo-app   : this will create a new deployemnt while keeping the old one up untill the new one comes up , 0 downtine.
				oc rollback dc/demo-app : this will roll back the deployment to the old version we terminated 
				
	standard phase of container creation : from pending -------> containercreating ---------> Running 	
-------------------------------------------------------DEPLOYEMENT CONFIG RESOURCE----------------------------------------


-------------------------------------------------------SERVICES AND ROUTES----------------------------------------
- Service is just like a front end part which will contain a public ip and port which is connected to the pods in the cluster.
- We know when we create / deploy an api we need an IP and port on which it will listen and sends the request to the pods in the backend.
- Theip and port assigned to the service is for internal pod communications . We can not access the service from outside world using this IP. So for that we have to create a route 


Steps : 
	-Create a pod first 
		oc create pod pod/pod.yaml
	-create the service 
		oc expose --port 8080 pod/hello-world-pod
		
***VVIMP : - This will create a service showing you the ip and port which is exposed internally inside the cluster and not access from internet.

		
	- to access the first pod create another pod in the same cluster and ping from second to first pod
		oc create pod pod/pod2/yaml
		oc status  ------------------------------- > you will get the service ip and address of first pod
		oc rsh pod/hello-world-pod-2
			: wget -qO- ipaddressoffirstpodservice:port
		

	- to access the first pod using env variable 
		command : env ------------ show all env inside pod terminal 
				  wget -qO- $HELLO_WORLD_POD_PORT_8080_TCP_ADDR:$HELLO_WORLD_POD_PORT_8080_TCP_PORT  : env used to get the response from first pod

	- Cretae a ROUTE  to access it from outside a cluster means from internet 
		oc new-app quay.io/practicalopenshift/hello-world --name demo-app --as-deployment-config 
		oc expose service/demo-app ---- this will expose the service to internet by assigning it a domain name
		oc status : -------- will show you the service domain name url us it to do get the response.
		oc get -o yaml route/demo-app
		oc get -o yaml svc/demo-app
		oc get -o yaml build/demo-app
		oc get -o yaml dc/demo-app
	above get -o yaml commands are to get the yaml representation of those in a file 

	Services uses label selectors under selector.app to route the traffic to backedn pool .

quay.io/practicalopenshift/hello-world@sha256:2311b7a279608de9547454d1548e2de7e37e981b6f84173f2f452854d81d1b7e

-------------------------------------------------------SERVICES AND ROUTES----------------------------------------



-------------------------------------------------------CONFIG MAPS ----------------------------------------
- ConfigMap holds configuration data for pods to consume.
- Config maps are used in case of we are deploying the apps in different regions dev , stage , productin etc 
- These configs maps are used by the pods. 1MB limit.
- oc explain configmaps 
- oc explain configmaps.data

	- create config map   : configmaps are on globall level , it can be applied on all the pods in the clusters.
		oc create configmap message-map --from-literal MESSAGE="HELLO FROM CONFIGMAPS"
		oc get configmaps
		oc get -o yaml cm/message-map
		
	- use the config map we created in above steps.		: command to apply the map variable to the demp-app using which is MESSAGE
		oc set env dc/demo-app  --from cm/message-map
		oc set env dc/hello-world  --from cm/message-map		
		
	- create a config map using a file 
		echo "Hello from new config map " >> MESSAGE
		oc create configmap file-map --from-file=MESSAGE
		oc get cm/file-map
		oc get -o yaml cm/file-map
				
				yaml file comes as below : 
				---------
				apiVersion: v1
				data:
				MESSAGE: "\"Hello from new config map \" \r\n"        ---------------> this is important . rest of the values in configmaps are just metadata
				kind: ConfigMap
				metadata:
				creationTimestamp: "2021-12-16T10:42:36Z"
				name: file-map
				namespace: meet14764-stage
				resourceVersion: "581158858"
				uid: 3033e676-9733-46d0-b6cd-caa6f27d978d
				---------
			oc set env dc/hello-world --from file-map		
			oc create configmap file-map --from-file=MESSAGE.txt -----------> pods is the folder that contains the files
			
			oc get -o yaml cm/file-map 
			apiVersion: v1
				data:
					MESSAGE.txt: "\"hello from config map file \" \r\n" --------> here the name of the key will not match the key used in hello-world app.
					kind: ConfigMap
					metadata:
					creationTimestamp: "2022-07-01T10:29:16Z"
					name: file-map
					namespace: meet14764-dev
					resourceVersion: "1455888886"
					uid: d7df1703-f3d9-44d2-84e5-e52a1bbc0c14
					
			oc create configmap file-map-2 --from-file=MESSAGE=MESSAGE.txt  ---------> this will update the key value to only MESSAGE		
			data:
				MESSAGE: "\"hello from config map file \" \r\n"
				kind: ConfigMap
				metadata:
				creationTimestamp: "2022-07-01T10:53:38Z"
				name: file-map-2
				namespace: meet14764-dev
				resourceVersion: "1455942379"
				uid: dbf5af7e-83a4-477f-b0fd-44c99309dbea
			oc set env dc/hello-world --from file-map-2	
			
	- create a config map using all files in a directory 
		oc create configmap pod-example --from-file pods -------------------------------> this will create config maps using directory
		
			apiVersion: v1
				data:
				pod.yaml: "apiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: hello-world-pod\r\n
					\ labels:\r\n    app: hello-world-pod\r\nspec:\r\n  containers:\r\n  - env:\r\n
					\   - name: MESSAGE\r\n      value: Hi! I'm an environment variable\r\n    image:
					quay.io/practicalopenshift/hello-world\r\n    imagePullPolicy: Always\r\n    name:
					hello-world-override\r\n    resources: {}\r\n"
				pod2.yaml: "apiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: hello-world-pod-2\r\n
					\ labels:\r\n    app: hello-world-pod-2\r\nspec:\r\n  containers:\r\n  - env:\r\n
					\   - name: MESSAGE\r\n      value: Hi! I'm an environment variable in pod 2\r\n
					\   image: quay.io/practicalopenshift/hello-world\r\n    imagePullPolicy: Always\r\n
					\   name: hello-world-override\r\n    resources: {}\r\n"
				service.yaml: "apiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: hello-world-pod-service\r\nspec:\r\n
					\ selector:\r\n    app: hello-world-pod\r\n  ports:\r\n    - protocol: TCP\r\n
					\     port: 80\r\n      targetPort: 8080\r\n"
				kind: ConfigMap
				metadata:
				creationTimestamp: "2022-07-01T11:03:45Z"
				name: pod-example
				namespace: meet14764-dev
				resourceVersion: "1455964920"
				uid: 5177308f-b22a-4913-ae66-07d7137962c3
				
-------------------------------------------------------CONFIG MAPS ----------------------------------------


-------------------------------------------------------SECRETS ----------------------------------------
- create secrets  generic or opaque secrets   ------ its totalyy similar to configmaps but the data stored as base64 encoded 

below command is used to create generic secrets
	oc create secret generic message-secrets --from-literal MESSAGE="SECRET MESSAGE"
	oc get secrets
	oc get -o yaml secrets/message-secrets
		Output : 
			apiVersion: v1
			data:
			MESSAGE: U0VDUkVUIE1FU1NBR0U=
			kind: Secret
			metadata:
			creationTimestamp: "2021-12-16T12:38:09Z"
			name: message-secrets
			namespace: meet14764-stage
			resourceVersion: "581479490"
			uid: 76b5e4ba-6616-4611-9110-2ddf2c5e27b9
			type: Opaque

	oc set env dc/demo-app --from secret/message-secrets --------> aplying the secrete message to env variable 


-------------------------------------------------------SECRETS ----------------------------------------


-------------------------------------------------------ImageStream & imagestreamtag ----------------------------------------
Imagestreamd 'and imagestreamtag are similar to what we studied as images and image tags .
 Command to create IS:
	oc get is
	oc delete is/demo-app : to delete the image stream 
	oc import-image --confirm quay.io/practicalopenshift/hello-world 
	oc new-app meet14764-stage/hello-world --as-deployment-config  ------------ after importing the image stream we can use that only and no need to create a new one , OC will create rest of the resource lik e, svc , dc , bc and route if required.
	
 Adding more IStags
    oc tag quay.io/practicalopenshift/hello-world:update-message hello-world:update-message
	
 Pushing the private image to quay.io	
	# Remote Tag syntax
	<host name>/<your username or Repository>/<image name(:tag)>
	
	# Load environment variables from credentials.env
	source credentials.env
	
	# Building an image with a remote tag
	docker build -t quay.io/$REGISTRY_USERNAME/private-repo .
	
	# Log into a registry
	docker login <hostname>
	
	# Log into quay.io
	docker login quay.io
	
	# Push (send) an image to a remote registry
	docker push <remote tag>
	
	# Push the image to Quay
	docker push quay.io/$REGISTRY_USERNAME/private-repo


 Downloading and deploying the private image 
    # The command to import the private image (won't work without extra auth steps)
	oc new-app quay.io/$REGISTRY_USERNAME/private-repo
	
	# You may need to run this command 
	source credentials.env
	
	# Create a Docker registry secret
	oc create secret docker-registry \
	demo-image-pull-secret \
	--docker-server=$REGISTRY_HOST \
	--docker-username=$REGISTRY_USERNAME \
	--docker-password=$REGISTRY_PASSWORD \
	--docker-email=$REGISTRY_EMAIL
	
	# A touch of secrets magic
	# This command links the secret to the service account named "default"
	oc secrets link default demo-image-pull-secret --for=pull
	
	# Check that the service account has the secret associated
	oc describe serviceaccount/default
	
	
	# The same image from the start should work now
	oc new-app quay.io/$REGISTRY_USERNAME/private-repo

 

-------------------------------------------------------ImageStream & imagestreamtag ----------------------------------------

-------------------------------------------------------BUILD config  ----------------------------------------
Its similar to when we run Docker build command using the src code. It create an image and use that further .

Create build config  : Build config logs if you will check you will find the logs showing the dockerfile line of codes running step by steps and creating an image 

	command : 
		- To create a build config 
			oc new-build https://gitlab.com/practical-openshift/hello-world.git
			oc get build
			oc get -o yaml bc/hello-world >> bc.yaml
			oc logs -f bc/hello-world ---------------------- to see the logs for bc for dockerfile LOC run.
			
		- To start an existing build vconfig .
			oc start-build bc/hello-world  ----------------- this will trigger the build config again to create an image or in open shift language image stream .
			oc get build -------------------------------------- then check the latest build you started 
			oc logs -f build/hello-world-3 -------------------- this is build name 
			  you can check the logs 
			  
			oc start-build is used in case we want to rebuild the image getting error before.
			oc get is/hello-world ------------ this will show the image builded by the start-build .	
		
		- To cancel the build : 
			oc cancel-build bc/hello-world

-------------------------------------------------------BUILD config  ----------------------------------------

-------------------------------------------------------Webhook for open shit build   ----------------------------------------

Webhooks are http endpoints which get triggers when a src system found any change in it .
														
																			|------------------------------------------------------------------------------------------|
																			|                                                                                          |
																			|                        		                                                           |
Client user ------Pushed code to ------> Git hub ------sends HTTP request --|-----> WEBHOOK -----trigger -----> build config ------> create -----> imagestream(image.) |
																			|                                                                                          |
																			|                                                                                          |
																			|			                    OCP PLATFORM                                               |
																			|------------------------------------------------------------------------------------------|
1. We are going to use generic webhook , for that we need generic secret for webhook to work 

	sequence of command  : on windows cmd
	
		oc get -o yaml bc/hello-world -------------> get the secret from build config under trigger tag in exported yaml 
		SET GENERIC_SECRET=H5gq7BUUavJh-qrn-nF1   ----------- > export the secret in ENV variable that gurbich code on left is the secret i too it from buildconfig yaml .
		ECHO %GENERIC_SECRET%
		oc describe bc/hello-world  ------------------- > in describe you will get the webhook links both github and generic.
		CURL -X POST -k https://api.sandbox.x8i5.p1.openshiftapps.com:6443/apis/build.openshift.io/v1/namespaces/meet14764-stage/buildconfigs/hello-world/webhooks/%GENERIC_SECRET%/generic  : this command will post a request to 	generic web hook for buildconfig and will trigger the build which you can see by looking at a new build pod running  cooooooooooooooooooool
		oc logs -f pods/hello-world-3 -------------------- > we can check the latest build running .
		
		
	IMPORTANT ****** In Minishift  , in build config , we can direclty add the webhook links for gitlab , github , or bitbucket . after adding the webhook got to you github settings or gitlab setting and in integration tab enter the webhook url and check the push event option and add webhook in it . Then test it .	
		
2. Post commits hook : means pushing the image build, post hook triger the builds created the image and runs the test.	
		oc new-build https://gitlab.com/harmeet2401/hello-world.git
		
		# Set a post-commit hook
		oc set build-hook bc/hello-world \
		--post-commit \
		--script="echo Hello from build hook"
		
		# Start a build to check it out
		oc start-build bc/hello-world
		
		# Check the logs output for "Hello from build hook"
		oc logs -f bc/hello-world
		
		# Set a failing build hook to observe the behavior for error 
		oc set build-hook bc/hello-world \
		--post-commit \
		--script="exit 1"
		
		# Check the events to see if it ran
		oc get events
		
		# Remove the build hook 
		oc set build-hook bc/hello-world \
		--post-commit \
		--remove


 
-------------------------------------------------------Webhook for open shit build   ----------------------------------------


-------------------------------------------------------BUILD FROM A BRANCH    ----------------------------------------

We know the command to create a build config using git repo 

oc new-build https://gitlab.com/practical-openshift/hello-world.git

But for building a particular branch for this 
oc new-build https://gitlab.com/practical-openshift/hello-world.git#update-message ---------------> upadte-message is the branch used


-------------------------------------------------------BUILD FROM A BRANCH    ----------------------------------------


-------------------------------------------------------BUILD FROM A BRANCH subfolder    ----------------------------------------

We know that we can build a branch using above command 

oc new-build https://gitlab.com/practical-openshift/hello-world.git#update-message ---------------> upadte-message is the branch used

We can build a subfolder project using 

oc new-build https://gitlab.com/practical-openshift/labs.git  --context-dir hello-world-go ---------------> hello-world-go is the folder in it .



-------------------------------------------------------BUILD FROM A BRANCH subfolder   ----------------------------------------


---------------------------------------------------Post build hook----------------------------------------------------------------

bC created -----> build starts ----> build runs -----> build completes ---------> post build hooks ----------> pushed image to IS

How to configure post commit build hook > 
steps : 
	1. first create a bc : oc new-build gitrepolnk 
	2. second : run command : oc set build-hook bc/hello-world --post-commit --script="echo hello from post build hook"
							  oc set build-hook bc/hello-world --post-commit --script="exit 1"
							  oc set build-hook bc/hello-world --post-commit --remove
	3. third : oc start-build bc/hello-world  : it will show you the logs in which just before pushing the image to IS it will print the echo command line 


---------------------------------------------------Post build hook----------------------------------------------------------------


------------------------------------S2I (source to image ) scripts --------------------------
There are two scripts that openshift use to convert the source code to a rnnable image and it will remove the need to create a dockerfile.

Scripts : 
Assemble : used to build image 
RUN  : used to run cmd instructions 

provides openshift advantages
remove dockerfile dependency
can use the existing builder images 


		case 1 : 
			OCP integrates the S2I scripts as part of the new-app command 
				oc new-app https://gitlab.com/practical-openshift/labs.git --context-dir s2i/ruby --as-deployment-config
			step you will find in build config  :RUN /usr/libexec/s2i/assemble
			wowwww there was no need of dockerfile . It took advantage of S2I ruby assemble scripts 
		
		
		Q .how this worked , how ocp detects to use ruby assemble script for above new-app holding ruby code. ?
		
		- Its worked like this using S2I Auto-detection
			
														----------------------			No
			ocp ------------------>check ------------------ dockerfile exists? ----------------> source strategy is used 
														----------------------
																|
																| Yes
																|
																|
														Docker strategy is used 
														
				
		
		- In docker strategy it will run the docker file as per the locs writteren 
		- IN source code strategy ocp will check for the specfic files example in case of java it will look for pom.xml and in case of ruby it found config.ru. so it took ruby assemblescript in first place to execute.
		
		
		Case 2 : How to explicitly tell the ocp its a Ruby image or java , or python or .net or nodej
		
			Command 
				oc new-app ruby~https://gitlab.com/practical-openshift/labs.git --context-dir s2i/ruby --as-deployment-config  --- > used ruby keyword with ~ tilda
				oc new-app python~https://gitlab.com/practical-openshift/labs.git --context-dir s2i/ruby --as-deployment-config  --- > used python keyword with ~ tilda this fails for a ruby project as usual 
				
				
	*****NOTE******	we can customize the assemble scripts in project folder .s2i/bin . if a script is present there then its our and we are overriding it  , if its empty then ocp will call the defaul assemble script for it .		
	
	
	
		Case 3 : Override the assemble scripts. We can avoid wrting a dockerfile as we have assemble scripts.
			1. While writting our own assemble always call the default script .
			2. we need to create .s2i/bin folder in our project to set the override.
			
			sample assemble
				#!/bin/sh

				echo Hello from before assemble script 
				
				# Call the original script
				/usr/libexec/s2i/assemble
				
				echo Hello from after assemble script

			3. oc new-app https://gitlab.com/practical-openshift/labs.git#add-s2i-overrides --context-dir s2i/ruby --as-deployment-config
		
			
------------------------------------S2I (source to image ) scripts --------------------------

------------------------------------volumes --------------------------
Filesystem mounted on pods 
    : configmaps , secrets
	: harddisk 
	: its portable 
	: we can attach and detach the volumes
	
types :
		Empty dir volumes : not persistence and is removed when pods deletes. not used mostly . But it can retain the dtaa in emptydir when pod restart
		
		1. Steps : 
			oc new-app quay.io/practicalopenshift/hello-world  --as-deployment-config 
			oc set volumes dc/hello-world --add --type emptyDir --mount-path /dir-demo-hello-world
			
			This will add below elements 
				VolumesMounts : 
			      - mountpath : /empty-dir-demo
				    name : Volumee-dhgm
					
				Volumes : 
                - emptyDir : {}
			      name : Volumee-dhgm
				  
			oc rsh pod/hello-world-pod 
				ls -ltr 
				check for the   /empty-dir-demo
				cd  /empty-dir-demo
				touch "thshskksks sks sks" >> test.txt 
				ls -ltr 

		2. Configmaps as a file in volumes
			oc create configmap cm-volumes --from-literal file.txt="this config is for volumes"  ------> file.txt is the key you can mention anytin .
			oc set volumes dc/hello-world --add --configmap-name cm-volumes --mount-path /cm-dir
			oc  rsh podname 
				 &:/ check for the cm-dir folder existing in the pod or not ?
				 
			
		3. Storage  : We are talking about the persistence storage volume that we claim for our pods.
			- first go to storage 
			- enter the name of the storage claim as we did in optum named eemsdev
			- enter the size to be claimed . Now here one more catch the claim we are doing it out of the storage attached to the cluster can be of any plugin , AWS EBS , azure volume , file system local , etc 
			- click save 
			- go to you pod 
			- in Deployement config click on add storage 
			- select the storage claim name created before 
			- enter the folder you want to be created 
			- save 
			
			Now this claimed storage is the persistence volumes accessible to all the pods who got that added as a storage .
			The data in that folder will be retained even if the pod is deleted.

------------------------------------volumes --------------------------


-----------------------------------------typesofpersistentvol----------------------------------
Kubernetes volumes page and some of the popular explain command for some resources created when new-app command ran 
command that create below resource : oc new-app https://gitlab.com/practical-openshift/hello-world.git --as-deployment-config
oc status : 
				svc/hello-world - 172.30.61.85:8080
					dc/hello-world deploys istag/hello-world:latest <-
						bc/hello-world docker builds https://gitlab.com/practical-openshift/hello-world.git on istag/golang:alpine   ---- extra step from above command we did 
						not built yet
							deployment #1 waiting on image or update
							
							
							
							
oc explain deploymentconfig 
oc explain buildconfig 
oc explain service
oc explain pod
oc explain cm  ---------------- configmap
oc explain secret
oc explain is  -------------- image stream 
oc explain istag ------------- image stream tag 
oc explain persistentvolume.spec ----------- will give all persistentvolume spec attributes
-----------------------------------------typesofpersistentvol----------------------------------


-----------------------------------------Advance Deployement config ----------------------------------

BIGGER AUTOMATION PROCESS FOR OPENSHIFT   : check for the image saved in git repo image name : OpenshiftAutomation Steps.png



1. CODE CHANGE TRIGGER AUTOMATION : ------------

CLIENT ---PUSHCODE--> GITHUB -----------> WEBHOOK  -----triggr -----> BUILD CONFIG -----create---> BUILD -----|
																											  |
																											  |
																											  |
																											  |
																											  |
										New Image  ---------------------------------create -------------------|
											|
											|
											|
											|-------changes -----------ISTAG--->image change trigger's ---> DC ---Trigger Rollout a new Dc 



NOTE : MEAN WHEN EVER WE PUSH THE CODE TO GIT , GIT WILL TRIGGER A WEB HOOK THAT WILL TRIGGER A NEW BUILD AND CREATE A NEW IMAGE OUT OF THAT BUILD , THEN IMAGE CHANGE TRIGGER IN Deployement CONFIG IS CONTINUOUSELY MONITORING THE IMAGE CHANGE , SO CHANGED IMAGE WILL TRIGGER A NEW DEPLOYEMENT AND NEW PODS COMES UP WITH NEW IMAGE CHANGES.		

NOTE  : WHENEVER YOU ARE LOOKING TO AUTOMATE THE TRIGGER FOR AN IMAGE CHANGED , CODE CANGE , CONFIG CHANGE ALSWAYS DO A DESCRIBE COMMAND 

EXAMPLE  IN CASE OF WEB HOOK ALWAYS FOR CODE CHANGE DO DESCRIBE COMMAND ON BUILD CONFIG  TO SEE THE WEB HOOK INFO , BECAUSE WE ARE ROLLING A BC
EXAMPLE IN CASE OF CONFIG CHANAGE , IMAGE STREAM CHANGE  ALWAYS DO DESCRIBE ON DEPLOYMENT TO SEE THE TRIGGER  , BECAUSE WERA ROLLING A DC  							
		
		
		
		
2. CONFIG CHANGES TRIGGER AUTOMATION : ------------------------

DC YAML CHANGES --------->NEW DEPLOYMENT ROLLOUT ------> NEW POD COMES UP 

This happens when we attach a new volumens , config or secret map .

	- Now we will do below steps to re-trigger the DC deployemnetconfig pod.
			1. Create new app : oc new-app githubrepo --as-deployment-config
			2. update the DC created in above command by either changing the ENV varible ot by changing the volume 
			3. oc set volumes dc/hello-world --add --type emptyDir --mount-path /dir-demo-hello-world
		
				above steps will create a new DC pod and terminate it once done.
	
	- How to modify the triggers as we know new-app automatically sets the trigger for you 
			. oc set triggers dc/hello-world ---- to get the trigger list 
			. oc set triggers dc/hello-world --remove --from-config ----------- to remove the triggers . this will make the Auto flag as false.
			. oc set triggers dc/hello-world  --from-config -------------- to readd the config change trigger.
			. oc set triggers dc/hello-world --remove --from-image hello-world:latest --------- this will remove the trigger for image change..
			. oc set triggers dc/hello-world  --from-image hello-world:latest  -c hello-world --------- this will add the image change trigger.
		


-----------------------------------------Advance Deployement config ----------------------------------


-----------------------------------------Deployement strategy ----------------------------------
two strategy for this course : 
	Rolling  : start a new one  , once comes up , stop the old one.  low downtime
		- oc rollout latest dc/hello-world
	
	Recreate : start a new one and bring all old pods down . This increase the downtime of the pods.
	
 1. Configure pre-deployemnets hook : 
		# Watch the pods
		oc get pods --watch
		
		# Trigger a deployment
		oc rollout latest dc/hello-world
		
		# Add the deployment hook
		oc set deployment-hook dc/hello-world --pre -c hello-world -- /bin/echo Hello from pre-deploy hook
		
		# Check the definition
		oc describe dc/hello-world
		
		# Roll out again to see the changes
		# You should get a hello-world-4-hook-pre pod
		oc rollout latest dc/hello-world
		
		# Check events
		oc get events 
		
2. Configure recreate deployemnets strategy : for this we have edit the deployment config manually uusing below command 

		- oc edit dc/hello-world  : change the strategy to recreate , then
		- oc rollout latest dc/hello-world : this will show you a recreationg process of app pod in other window for (oc get pods --watch) command 






-----------------------------------------Deployement strategy ----------------------------------


-------------------------------liveness and readiness probes ------------------------------------


Just like health check for the pods running on OCP .
This will allow auto check of health and restart if down 

1. Readiness probe : this will fail only in case pod is not in a condition of receiving traffic.
2. Liveness probe : this happen in case of app is down and pod is also down , in this the pod will not restart. This probes if fails will restart the pods for health well.



	probe check : 
		http get probe  : for rest api .
		TCP :  TCP socket connection check for non-rest api .
		Command :for no- rest api  , command execution probe is configured inside pod which keeps on running for health check  
		
	paramters : 
		no of tries 
		time betweek replies .
		
	- configure the liveness probe  : for restaring pod fall into failure states.
		. oc set probe dc/hello-world --liveness --open-tcp=8080   :     Liveness:           tcp-socket :8080 delay=0s timeout=1s period=10s #success=1 #failure=3
		
	- Set the probe incorrectly use wrong port , as our hello-world is listens on 8080 . we will use 8081 , it will fail and restart the pods.	
		. oc set probe dc/hello-world --liveness --open-tcp=8081

-------------------------------liveness and readiness probes ------------------------------------


-------------------------------Scaling  ------------------------------------
replication controller : 1 by default 
	
	1. Manually command level scaing 
		- oc new-app quay.io/practicalopenshift/hello-world  --as-deployment-config
		- oc scale dc/hello-world --replicas=3
	2. HPA : horizontal pod autoscaler. : 
			Scaling as per the resource utilization of the pod. means CPU % or memory  %used .
			Usage ratio  : used currently / Threshhold    : ===== example 200 millis/100 millis  : 2
			No of pods to be running : no of pods running * usage ration  : 4 * 2 = 8 
			
			and vice versa 
			
		Steps to create HPA : 
			oc autoscale dc/hello-world --min 1 --max 10 --cpu-percent=80
			oc get hpa  -------------- will show you the targets 0%80% minpods and maxpods ,replicas and AGE
			oc delete hpa 
			oc rsh podname 
					 : run : yes > /dev/null &  ----- 5 times 
					 : run : cat /proc/cpuinfo : to see the cpu utilization 
			oc get pods --watch ------------------------ you will see the pods scaling up as the utilization of cpu goes more that 80%		 
	
	

-------------------------------Scaling  ------------------------------------



-------------------------------Template : code as a service : using the template code to deploy the app on ocp servers ------------------------------------
We can use yaml and do all the stuf as we did above so far.

Three part sof template ::

		----------------------
			Preamble 
		----------------------	
		  Object resource lst 
		----------------------	
			Parameters
		----------------------

		How to create : 
			oc create -f hello-world-template.yaml  : upload to OCP servers . once this is uploaded we can update or use it to create deplpments , bc , svc , route etc 
			oc new-app hello-world  : this will create the pods similar .
			oc new-app hello-world -p MESSAGE="HELLO FROM OTHER PARMS"
			
		Processing template : Oc new-app command is not the only way to use template .
			oc process hello-world  : this will show the template as a json  
			oc process hello-world -o yaml >> proces.yaml 
			oc create -f process.yaml 
			
		Processing a file presnt locally without uploading the file to ocp server.	
			oc process hello-world -o yaml -f hello-world-template.yaml
			oc new-app hello-world -f hello-world-template.yaml -p MESSAGE="HELLO FROM OTHER PARMS2 !!!!!! "
		
		Custom our own template : 
			# Get YAML for existing objects on the OpenShift server
			# The dc,is... syntax lists the types of resources that you would like to export
			# Add hpa or any other type if you need them
			oc get -o yaml dc,is,bc,svc,route --export
			
			# Save output to a file
			oc get -o yaml dc,is,bc,svc,route \
			--export \
			> test-template.yaml
			
			# Open it up
			vi test-template.yaml
			
			Steps for a custom template:
			1. Change the items property to objects
			2. Change kind from List to Template
			3. Add a name property to the metadata section
			4. Remove status from each resource
			5. Remove most of metadata except for name, labels, and annotations
			6. Remove any automatically-assigned resources such as service Virtual IPs and Route hosts
			7. (optional) Add template parameters 
		
		How to use built -in templates : 
			oc get template  -n openshift

		In MInishift we can see multiple app catalog to select from . you can add more by searching the template and just import it and save it .
		   Search In openshift origin repository 
-------------------------------Template : code as a service  ------------------------------------

---------------------------------Custom template ------------------------------------------------

 - Means creating a template yaml file for the entire arch so that we can deploy all the microservice using only one template yaml file in the catalog
 - when ever you use the yaml file from ocp server for existing ocp resource like IS , dc , bc , secrets, services , routes etc . post copying it keep the spec as it is rest remove the uneccessory details from metadata.
 - once copying is done , create a new project in ocp , 
	login to that project 
		use command : oc create -f template.yaml -n namespacename


---------------------------------Custom template ------------------------------------------------


######################################################################################################################################################

 Quick intro on Docker and kubernetes :
 
- Docker is the containerization tech used to create containers and allow easy dev and operation hadshake without any delay .
- Contains are light weight processes running it its own isolated space and share the same OS as underlying hostmachine .
- Images are the package , template that we used which contains the underlying base image , liberaries and actual build file for your application to run .
- Containers are the running instances of the images that runs on a single host .
- Tool need to run docker containers 
	Docker engine . : This engine of process helps is running the containers with the help of docker daemon thread that send or receives the request from docker engine that rsend/receives the request from the clienst 
- Kubernetes : Theere should be tool in place in case of performance hit happens on the app architecture , this tool might have the functionality to scale up and scale down the contains runing on one host machine or multiple host machine automatically.
So kubernetes is the container orchestration tool helps in managing the infra and instance level management for our apps arch.	
- Dockerfile commands : https://kapeli.com/cheat_sheets/Dockerfile.docset/Contents/Resources/Documents/index


Q. Create a hello world java rest api listening on port 8080 , build it using dockerfile and deploy it on openshift .
We need  : https://codefresh.io/docker-tutorial/create-docker-images-for-java/
	1. Rest api java project 
	2. Use the maven tool to build the jar file or the build runable file. 
	3. Create a Docker file which contains the base jdk image , copy command to copy the jar to the container flder and CMD to run the jar and EXPOSE the port for the host port : container port mapping 
	4. Test using the command : docker build -t meet14764/myapi:1.0  . ------------ this will rename the image to meet14764/myapi:1.0 where meet14764 is my repo name in docker hub repository.
	5.  Use the : oc new-app quay.io/meet14764/practicalocp:2.0 --as-deployment-config  --------working 
		or 		  oc new-app meet14764/ocprepo:myapi-1.0.snapshot --as-deployment-config   ------ some issue
				  
	6. check the app in OCP clusters.
	
	
	Docker quay.io push and pull command s : https://docs.quay.io/solution/getting-started.html	
	docker tag meet14764/myapi:1.0 quay.io/meet14764/practicalocp:2.0
	docker push quay.io/meet14764/practicalocp
	
	
	Note : DON'T ADD THE BUILD TOOL LIKE MVN AS A LAYER IN YOUR DOCKER FILE IT WILL INCREASE THE SIZE OF THE IMAGE AND MAKE THE STORUING COST MORE.
	
	
	
	docker pull quay.io/meet14764/practicalocp:2.0
	docker pull meet14764/ocprepo:myapi-1.0.snapshot
	
	
Helm Charts : What is a helm chart?
A Helm chart can be thought of as a Kubernetes package. Charts contain the declarative Kubernetes resource files required to deploy an application. Similar to an RPM, it can also declare one or more dependencies that the application needs in order to run. Helm v3 is based on a client-only architecture.
