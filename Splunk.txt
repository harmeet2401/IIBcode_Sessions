Splunk -----------------------------------------
https://www.splunk.com/
Splunk account : 
meet14764
Gogi@12345


Support community : Community.splunk.com

Splunk documentation : docs.splunk.com

What is splunk 
- Splunk is an IT operations tool that consumes machine-data(any system , app on -premises or on cloud ) in real time. Most BI tools do not consume and analyze data in real time.

- Splunk is used to turn or convert unstructured data into something readable and present it in human readable format .

- Splunk main component is INDEX , consider INDEX as the factory and raw data is pushed to the INDEX(factory) where the inspector inspect the data and organise those into the events . These events are marked with time stamps and can be picked by the user from the splunk web app search bar

- We can use wild card like below in search bar : the search keyword i not case sensitive
	fail*
	failed
	faiLed
	FAILED
	failed OR password 
	failed NOT password 
	failed and password
	"chris password"  ----------------- to search for the exact string use quotes
	"\chris password \" --------------- to search for the keywords that contains quotes use backslash.
	
	

-is  a powerful tool like Splunk to investigate and solve a complex IT problem.

- it stores the data it consumes in harddisk of the system on which the instance is running .
Data storage in Splunk : 

-splunk transforms incoming data into events stores in idexes(which are spaces called buckets in harddisk).
-Store as a single row data for each event and have its own metadata .
-add default field to each events like  source, sourcetypes  , timestamp , host.
	: source : means file , app , cloud webservice etc 
	: sourcetype : format of data 
	: host : source ip address
	: timestamp : current timesatamp
	


		Incoming data -----transformsinto each -----------> event ---------->stored on indexes(buckets)-------->On DISK
- 5 types of buckets in which the data is ditributed as per the requirement 

	1. hot (read / write ) : this is where we interact with the data 
	2. warm (read ) : data rolled from hot 
	3. cold (directory changed ) : in this the directory is changed for the data .
	4. Frozen(Deletes or archive the data )
	5. thawed : data is restored from archive in here .
	
	
- Licensing 
	purchaing model : 
	
		a. standard
		b. enterprise
		c. Sales trial
		d. Dev/test
		e. free 
		f. Industrial IOT 
		g. Forwarder 
		
- splunk apps :  collections of splunk configuration files . Download it from splunkbase.com
	provides below features : 
	  visualization 
	  analysis
	  Reports % Dashboard 
	  User Interface 
- Splunks Add on : is a subset of splunk 
	  Data enrichment 
	  tags 
	  data models
	  Data sets

- we can directly go to splunk.com , create an account download the software enterprise windows 64 msi install it and add the admin username and password and start using that by uploading the data 

- We can then go to splunk enterprise installer we install on windows and then use it to download an install splunk app. Splunk app is the collection of the splunk configuration files and using subset of the splunk fetaures.


Windows : 
C:\Program Files\Splunk\var\lib\splunk\defaultdb\db\hot_v1_0\rawdata : for splunk data where the data we uploaded for search to test is stored 


Common splunk flow for incomgin data 
- IPIIS : input  --> parsing --> indexing queue --> indexing  --> search 



1. Input : 
	a. File and directory inputs :
		- Monitor files and directory : local or remote file/directory monitoring. Monitor compressed files . 
		- upload files to splunk , used for one time anayalis
		- Monitor no handle : monitors only the windows host system files which auto rotate .
	b. Network input : 
		Data over TCP or UDP : ---> like sending syslogs to splunk
		Data from SNMP 
	c. Windows Input : 
		Windows event logs forwarding , error logging , system boot logs etc to splunk . perfmon logs to splunk 
	d. App sending data over HTTP to send the events or logging data to splunk . as we did in using the splunk dependencies for oour rest api and pass the logging data to splunk .
		link for spring boot and splunk integration  : https://stackoverflow.com/questions/61426758/splunk-integration-with-spring-boot
		

	We can set up inputs means start or pushing data to splunk using : 
	CLI : splunk add monitor command 
	Splunk web : Add data : upload data 
	Splunk app 
	java integration using the jar it provides . configure the username and passwords.
	
	
2. Forwarder : 
		Universal : Only sends the data from source to receiver  , does not apply indexes and searching . 
		Heavy :  not only forward the data but also transforms the dataa and need full set of installation .

	how to configure both forwarder ? Forwarder are just like we create a forwarder which will keep on monitoring a file , directories , tcp or udp port , an app server , a windows log events directory , error directory etc .

	a. We can download the setup for universal forwarder and install it and configure it in same way as we did in splunk web
		
		
		
	b. Below steps are done on splunk web .	
		steps : for receiver 
			1. open splunk local application on web 
			2. click on setting 
			3. Under Data ----> forwarder and receiver 
			4. select configure recever 
			5. Enter the port and save 
	
		steps : for forwarder 
			1. open splunk local application on web 
			2. click on setting 
			3. Under Data ----> forwarder and receiver 
			4. select configure forwarder 
			5. Enter the IP : port and save 
		steps to add data File and directory inputs :
			1. Add input 
			2. file and directories
			3. select the folder and olaaa 
		Go back to home page and search and 	search for host=splunkmainlocal 
------------------------------------------------------------------------------------------------	
3. 	ADD DATA : BELOW OPTIONS ARE THERE : 

Local Event Logs
Collect event logs from this machine.

Remote Event Logs
Collect event logs from remote hosts. Note: this uses WMI and requires a domain account.

Files & Directories
Upload a file, index a local file, or monitor an entire directory.

HTTP Event Collector
Configure tokens that clients can use to send data over HTTP or HTTPS.

TCP / UDP
Configure the Splunk platform to listen on a network port.

Local Performance Monitoring
Collect performance data from this machine.

Remote Performance Monitoring
Collect performance and event information from remote hosts. Requires domain credentials.

Registry monitoring
Have the Splunk platform index the local Windows Registry, and monitor it for changes.

Active Directory monitoring
Index and monitor Active Directory.

Local Windows host monitoring
Collect up-to-date hardware and software (Computer, Operating System, Processor, Service, Disk, Network Adapter and Application) information about this machine.

Local Windows network monitoring
This is an input for Splunk Network Monitor.

Local Windows print monitoring
Collect information about printers, printer jobs, print drivers, and print ports on this machine.

Scripts
Get data from any API, service, or database with a script.

Powershell v3 Modular Input
Execute PowerShell scripts v3 with parameters as inputs.

Splunk Secure Gateway
Initializes the Splunk Secure Gateway application to talk to mobile clients over websockets

Splunk Secure Gateway Mobile Alerts TTL
Cleans up storage of old mobile alerts

Splunk Secure Gateway Deleting Expired Tokens
Delete expired or invalid tokens created by Secure Gateway from Splunk

Splunk Secure Gateway Role Based Notification Manager
Used for sending mobile alerts to users by role

Splunk Secure Gateway Enable
Determine if Splunk Secure Gateway core modular inputs should be enabled

Splunk Secure Gateway Metrics Collector
Collects metrics for Splunk Secure Gateway

Splunk Secure Gateway Registered Users List
Sync the list of registered gateway users

Splunk Secure Gateway Subscription Clean Up
Clean up expired subscriptions

Splunk Secure Gateway Subscription Processor
Process subscriptions and send visualization data to subscribed devices.

------------------------------------------------------------------------------------------------	

--------------------------SPL search processing language -----------------------

1. Search  pipeline : It goes like this 

			globae of raw data or events  | intermediate global transforms | get the readable format data |
			
			command1 | command 2 | search 
			
			Command1 search results will be fed to command 2
			
			'as' keyword will store the result sin the field name after as keywork and 'by' is used to sort the data as per the field writtern after the by keyword
			
			'search' keyword can further filter the results comming from command 2 and show it.
			
			Index , source , host and sourcetype are the strongest field we can use to filter the data 
			
			Above three process keep on happening untill we get the structured data in hand .
			
			NOTE : SEARCH COMMAND ALWAYS RETURN THE RESULTS IN reverse chronological order (from most recent to oldest) ORDER MEANS according to the time they occurred

2. Time : we can manage the time frames or using time stamps .
				So we can set the filter of time frames or zones to get the data out. its essential fild.
				
			- its looks for the event time stamp occured 
			- source name or file 
			- file modification time 
			- current system time stamp 
			- Time variables : 
				%c
				%F 



3. basic search  : 
	We can use bsic fields : will reduce the processing time .
		host 
		index 
		source and sourcetype 
		wildcards :  *ailed , tes* , id*  , usern*
		repeatable fields in each event .

	
	PIpe 
	
		sort 
		rename 
		
		
		example:  
		
		Note : space in between is AND 
		
		host=mysts.cl source=hstlogs  user*=(message=fail* OR message=loc*)
		| table _table  user message  ------------------------------------------------ will display only three column
		| sort -_time ------------------------------------------------ will sort by _time
		
4. Fields 	splunk auto discover fields 
	default fields 
	obvious key-value pairs 
	fields set  by apps

	Mode of field search : 
		fast , smart and verbose
		
	field extractions : 
		use regular expression 
--------------------------SPL search processing language -----------------------

COmmand : 

Top  : will give the most command value and top values using a diff colum or attribite value 
	host=main | top Industry_aggregation_NZSIOC by year
	
rare : is apposite of top command , it will give the least common value fields 

	 host=main | rare Industry_aggregation_NZSIOC by year
Stats : stats avg(kbps) by host 	
		
		avg(kbps)   host 
		654.99		host1.domain.com
		768.9		host2.dommain.com
		
		stats count(failed_login) by users
			
			failed_login    user
			43				james
			22				jacob
			
		host=main | stats count(Units) as cunit by Year | sort by year  : its means number of Units fields in a year and that year will be fetched by splunk 

		Year	cunit
		2013	4635
		2014	4635
		2015	4635
		2016	4635
		2017	4635
		2018	4635
		2019	4635
		2020	4635

---------------------------visualization : ----------------------

Reports and alerts need enterprose level license and in free license the report and alerts are disabled

Reports : means get the data fetch it and draft it into a file or table bar chars etc and generate it on hourly , mins days or monthly etc basis on the dashboard .
Alerts : is to send emails or message to the concerned user for a data reaching a certain threashhold.

1. Pivot tool : To generate report , dashboard and alerts without knowing the SPL language .
		pivot function : 
			- filters 
			- split by row 
			- split by col
			- colume value : its numberic involves sum etc.
		  By defulat it will show count of events .
		  It will show just like excel 
		  
- Demo : creating dashboard and will create data model 

- Pretty easy to generate the dashboard and generate the alerts just like we saw that in optum . The pivot is the tool which works of the data sets and help in sorting itand convert it into graphs and show it on the dashboard.

- The one we created : http://127.0.0.1:8000/en-US/app/search/splunk_internal : this dashboard we created using the existing data model which is the system generated data sets 

- To create a new data model for your data sets or data , click on setting  : data model -  new data model . and add events 
	: Post creating the data model and save it , we can open that data model in pivot and starts creating th alerts and dashboard for visualization we will create.
	
	
2. Deployment and forwarder management .
3. users roles and administrator 
	- Roles : 	
		admin
		power users 
		users 
		can _ dalete 
		splunk _system _role
	- Splunk administrator  : can cretae roles 
	
  	- Authentication types : 
		Local 
		LDAp 
		SAML 
		Scriptes SSO 
		
4. Configuration file : what ever you do online splunk it will write it to the splunkg cofiguration files .

    Configuration precedence maintain by splunk :
		system local directory 
		app local directory
		app default directory
		system default directory
		
5. knowledge objets : 
			Add knowledge to enrich the data .
			include fields , extraction , saved searches or etc 
		a. Saved searches 
		b. Field extraction 
		c. tags : assign name to the specific field and value combinations 
		d. event types : tagging an entire search string . Then we can use that tag everytime for this complete string search .
		e. Lookups : Add custom fields to the events from external sources . lik ecsv files.
		
	Demo of above knowledge objects : 
		1. Lookup tables :  is a csv file with 2 number of columns . OOh k just like we need to replace the value with other value. Lookups will help in mapping the values.
			create a csv file as below 
			
			test.csv : 
			log_level,column2
			INFO , INFO has been converted
			WARN , WARN has been converted

			Setting --> lookups --> add the file change the permission 

			index=_internal* log_level=warn | lookup test.csv log_level output column2 | table log_leve1 column2		
			
			

			


		